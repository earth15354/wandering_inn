{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELF ATTENTION\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = torch.nn.Linear(C, head_size, bias=False)\n",
    "query = torch.nn.Linear(C, head_size, bias=False)\n",
    "value = torch.nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # B, T, head_size\n",
    "q = query(x) # B, T, head_size\n",
    "wei = q @ k.transpose(-2,-1) # only transpose last two dimensions: B,T,16 @ B,16,T ---> B,T,T\n",
    "wei = wei * head_size ** -0.5 # we do this to prevent softmax from converging to a one-hot vector (control the variance at initialization)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T)) # mask to stop future from communicating with past\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # DELETE THIS FOR ENCODER BLOCKS\n",
    "wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "# FOR ENCODER-DECODER ATTENTION, ONLY QUERIES ARE PRODUCED FROM X, & KEYS AND VALUES ARE FROM ENCODER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandering_env_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60692694684ff9f8903ab0e76cc64d9fb44b12b4600cdc9238c61423a4072cca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
